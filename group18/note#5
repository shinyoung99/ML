# 기계학습 7주차 정리노트

201703205 김현준 202184032 안신영

gini

 불순도
50:50 불순도 1
지니값이 더이상 학습 안될 경우
depth가 중요하다
훈련은 결국 손실 함수 최소화
p plus or minus
미니 샘플 스플릿(미니멈)
비매개변수 or 매개변수(규제 매개변수)
결정트리는 어떤 모델인지 미리 정해지지 않는다.
일반화
MSE를 최소화하는 방향으로 최소화
과적합을 막기 위한 규제
결정트리로 하는 회귀분석의 성능은 좋지만 너무 민감하다
훈련 데이터를 더 좋은 방향으로 회전시키는 PCA 기법 사용 (8장)?(추후 알아볼것)

결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없다(반대로 선형 모델은 데이터가 선형일 거라 가정한다). 제한을 두지 않으면 트리가 훈련 데이터에 아주 가깝게 맞추려고 해서 대부분 과대적합되기 쉽다. 결정 트리는 모델 파라미터가 전혀 없는 것이 아니라(보통 많다) 훈련되기 전에 파라미터 수가 결정되지 않기 때문에 이런 모델을 비파라미터 모델(nonparametric model)이라고 부르곤 한다. 그래서 모델 구조가 데이터에 맞춰져서 고정되지 않고 자유롭다. 반대로 선형 모델 같은 파라미터 모델(aprametric model)은 미리 정의된 모델 파라미터 수를 가지므로 자유도가 제한되고 과대적합될 위험이 줄어든다(하지만 과소적합될 위험은 커진다).

훈련 데이터에 대한 과대적합을 피하기 위해 학습할 때 결정 트리의 자유도를 제한할 필요가 있다. 규제 매개변수는 사용하는 알고리즘에 따라 다르지만, 보통 적어도 결정 트리의 최대 깊이는 제어할 수 있다. 사이킷런에서는 max_depth 매개변수로 이를 조절한다(기본값은 제한이 없는 것을 의미하는 None). max_depth를 줄이면 모델을 규제하게 되고 과대적합의 위험이 감소한다.

DecisionTreeClassifier에는 비슷하게 결정 트리의 형태를 제한하는 다른 매개변수가 몇개 있다. min_samples_split(분할되기 위해 노드가 가져야 하는 최소 샘플 수), min_samples_leaf(리프 노드가 가지고 있어야 할 최소 샘플 수), min_weight_fraction_leaf (min_samples_leaf와 같지만 가중치가 부여된 전체 샘플 수에서의 비율), max_leaf_nodes(리프 노드의 최대 수), max_features(각 노드에서 분할에 사용할 특성의 최대 수)가 있다.

실습(결정 트리가 어떻게 예측을 만들어내는지 살펴보기)
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris

# Iris 데이터 로드

iris = load_iris()
X = iris.data
y = iris.target
from sklearn.tree import DecisionTreeClassifier

# Create a decision tree classifier with a maximum depth of 3

clf = DecisionTreeClassifier(max_depth=2)

# Train the decision tree model on a sample dataset

# Features: 꽃잎 길이 (petal length), 꽃잎 너비 (petal width)

# Target classes: Setosa, Iris-Versicolor, Iris-Virginica

X = [[2.0, 1.5],  # Sample 1: 꽃잎 길이 <= 2.45cm -> Setosa
[2.5, 1.5],  # Sample 2: 꽃잎 길이 > 2.45cm, 꽃잎 너비 <= 1.75cm -> Iris-Versicolor
[2.5, 2.0]]  # Sample 3: 꽃잎 길이 > 2.45cm, 꽃잎 너비 > 1.75cm -> Iris-Virginica

y = ["Setosa", "Iris-Versicolor", "Iris-Virginica"]

clf.fit(X, y)

# Predict the class for a new sample

new_sample = [[2.3, 1.6]]
predicted_class = clf.predict(new_sample)
print(f"Predicted class: {predicted_class[0]}")

from sklearn.tree import export_text
from sklearn.tree import export_graphviz
import graphviz

# 결정 트리 시각화

dot_data = export_graphviz(clf, out_file=None,
feature_names=iris.feature_names,
class_names=iris.target_names,
filled=True, rounded=True, special_characters=True)

graph = graphviz.Source(dot_data)
graph.render("iris_decision_tree")

# 텍스트로 결정 트리 규칙 출력

tree_rules = export_text(clf, feature_names=iris.feature_names)
print(tree_rules)

tree_clf.predict proba([[5, 1.511)
array([[0.
0.90740741, 0.0925925911)
tree_clf.predict([[5, 1.5]])
array([1])

카트 알고리즘은 결국 탐욕적 알고리즘

실습2

from sklearn.datasets import make_moons
Xm, ym = make_moons (n_samples=100, noise-0.25, random_state=53)
deep_tree_clf1 DecisionTreeClassifier (random_state=42) =
deep_tree_c112 DecisionTreeClassifier (min_samples_leaf-4, random_state=42)
deep_tree_clf1.fit(Xm, ym)
deep_tree_clf2.fit(Xm, ym)

실습3

from sklearn.tree import DecisionTreeRegressor
tree_reg DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg.fit(x, y)
tree_reg.fit(x, y)

연습문제 MOONS를 결정트리로 해결
맥스립무브값
cv값?
GRIDSEARCH CV

연습문제(중요**)

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {
'criterion': ['gini', 'entropy'],
'max_depth': [None, 10, 20, 30],
'min_samples_split': [2, 5, 10],
'min_samples_leaf': [1, 2, 4]
} #매개변수 설정

tree_classifier = DecisionTreeClassifier()
grid_search = GridSearchCV(tree_classifier, param_grid, cv=5 )# cv=폴드 수

X, y = make_moons(n_samples=1000, noise=0.4, random_state=42) #데이터 생성

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #훈련용, 검증용 분리,

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

y_pred = best_estimator.predict(new_data)
